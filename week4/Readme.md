# ğŸ“š Week 4 - Q-Learning and Solving MountainCar Environment

## ğŸš€ Overview

In Week 4, we shift our focus to **Q-Learning**, one of the fundamental algorithms in Reinforcement Learning.
Our goal is to "solve" the **MountainCar-v0** environment provided by **Gymnasium**, using discrete state-space
representation and Q-learning updates. This week provides a practical understanding of applying RL to real 
environments where the state and action spaces are continuous.

---

## ğŸ¯ Objectives

- âœ… Understand Q-Learning and its update mechanism.
- âœ… Discretize continuous observation spaces for classical RL algorithms.
- âœ… Implement Q-Learning for MountainCar.
- âœ… Analyze agent performance through training metrics and visualization.

---

## ğŸ“¦ File Structure
```
Week_4/
â”œâ”€â”€ q_learning_mountain_car.py   # Main Q-learning implementation for MountainCar environment.
â”œâ”€â”€ utils.py                    # Utility functions for state discretization, environment handling.
â””â”€â”€ analysis_plots.py           # Scripts for plotting training performance, rewards, Q-tables, and learning curves.
```
---

## ğŸ§  Concepts Covered

- **Q-Learning**:
  - Off-policy Temporal Difference learning algorithm.
  - Learns optimal action-value function (Q-function) for state-action pairs.

- **Q-Learning Update Rule**:
  Q(s, a) â† Q(s, a) + Î± * [ r + Î³ * max_a' Q(s', a') - Q(s, a) ]

- **Discretization of Continuous Space**:
  - MountainCar observations (position, velocity) are continuous.
  - We discretize these into bins to create manageable state representations for Q-learning.

- **Exploration vs. Exploitation**:
  - Îµ-greedy policy used to balance exploration of unknown actions with exploitation of learned best actions.

---

## âš™ï¸ Algorithm Flow

1. **Initialization**:
   - Discretize observation space into bins.
   - Initialize Q-table with zeros.

2. **Training Loop**:
   - For each episode:
     - Start at an initial state.
     - Choose action using Îµ-greedy policy.
     - Take action, observe reward and next state.
     - Update Q-table using Q-learning rule.
     - Decay Îµ over time for better exploitation.

3. **Evaluation**:
   - After training, evaluate the learned policy.
   - Plot episode rewards, Q-values, and visualize performance.

---

## ğŸ“Š Expected Outcomes

- Plots of cumulative rewards per episode.
- Q-value heatmaps showing learned policy.
- Understanding of:
  - How state discretization affects learning.
  - Impact of hyperparameters (learning rate, discount factor, Îµ-decay).
- Fully trained agent capable of solving MountainCar environment.

---

## ğŸŒ Useful References

- Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction.
- Gymnasium Documentation: https://gymnasium.farama.org/environments/classic_control/mountain_car/
- Grokking Deep Reinforcement Learning - Chapters on Q-learning and Value-based Methods.

---

## ğŸ’¡ Notes

- Experiment with hyperparameters like learning rate (Î±), discount factor (Î³), and exploration rate (Îµ).
- Track and visualize learning progression to better understand the dynamics of Q-learning.
